{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Used Python 3.5.2\n",
    "\n",
    "# Importing libraries\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "# library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# tensorflow for machine learning\n",
    "import tensorflow as tf\n",
    "#numpy for array\n",
    "import numpy as np\n",
    "import PIL.Image \n",
    "from PIL import Image\n",
    "# skimage image processing\n",
    "from skimage import io,transform,img_as_float\n",
    "# for reading and saving\n",
    "from skimage.io import imread,imsave\n",
    "# for Returning a 2-D array with ones on the diagonal and zeros elsewhere\n",
    "from numpy import eye "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function takes the source image and put the source image color on target image\n",
    "#without loosing any target image content. \n",
    "# So we can use for preserving the color of visual-art or add any color which we like\n",
    "#using source color image\n",
    "#This function uses the linear color transfer techniques and par tof the code from\n",
    "#https://github.com/ProGamerGov/Neural-Tools/blob/master/linear-color-transfer.py\n",
    "#referece from reserach paper https://arxiv.org/pdf/1606.05897.pdf\n",
    "\n",
    "def color_tranfer(target_img,source_img):\n",
    "    #loadng the source image as numpy array and do the normalization\n",
    "    source_img = np.float32(Image.open(source_img))/256\n",
    "    #loadng the target image as numpy array and do the normalization\n",
    "    target_img = np.float32(target_img)/256\n",
    "    \n",
    "    def change_color(target_img, source_img, eps=1e-5):\n",
    "    \n",
    "        #change the colour distribution of the target image to that of the source image\n",
    "        #using a linear transform\n",
    "        #getting mu of target image\n",
    "        mu_of_target = target_img.mean(0).mean(0)\n",
    "        #taking the difference of target image and mu of target\n",
    "        t = target_img - mu_of_target\n",
    "        #doing the transpose and reshape\n",
    "        t = t.transpose(2,0,1).reshape(3,-1)\n",
    "        Ct = t.dot(t.T) / t.shape[1] + eps * eye(t.shape[0])\n",
    "        #getting mu of target image\n",
    "        mu_of_source = source_img.mean(0).mean(0)\n",
    "        #taking the difference of target image and mu of target\n",
    "        s = source_img - mu_of_source\n",
    "        s = s.transpose(2,0,1).reshape(3,-1)\n",
    "        Cs = s.dot(s.T) / s.shape[1] + eps * eye(s.shape[0])\n",
    "        # applying pca technique\n",
    "        eva_t, eve_t = np.linalg.eigh(Ct)\n",
    "        Qt = eve_t.dot(np.sqrt(np.diag(eva_t))).dot(eve_t.T)\n",
    "        eva_s, eve_s = np.linalg.eigh(Cs)\n",
    "        Qs = eve_s.dot(np.sqrt(np.diag(eva_s))).dot(eve_s.T)\n",
    "        ts = Qs.dot(np.linalg.inv(Qt)).dot(t)\n",
    "        \n",
    "        matched_img = ts.reshape(*target_img.transpose(2,0,1).shape).transpose(1,2,0)\n",
    "        matched_img += mu_of_source\n",
    "        matched_img[matched_img>1] = 1\n",
    "        matched_img[matched_img<0] = 0\n",
    "        return matched_img\n",
    "    #Using the change_color function to get the output image\n",
    "    output_img = change_color(target_img, source_img, eps=float(1e-5))\n",
    "    imsave('output_name.png', output_img)\n",
    "    #returning the output image\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading VGG16 Model ...\n",
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "#donwload the VGG16 model from here:http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-16.mat \n",
    "#store under ./data\n",
    "#We are now going to use the pretrained VGG16 model \n",
    "import vgg16\n",
    "vgg16.data_dir = 'vgg16/'\n",
    "# download model in vgg16 folder\n",
    "vgg16.maybe_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure the VGG16 model is stored in VGG_PATH\n",
    "VGG_PATH = 'data/imagenet-vgg-verydeep-16.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to load image and resize it\n",
    "def load_image(filename, max_size=None):\n",
    "    #opening the image\n",
    "    image = Image.open(filename)\n",
    "    if max_size is not None:\n",
    "        # find the rescale-factor\n",
    "        # keeping the proportion between height and width.\n",
    "        fact = max_size / np.max(image.size)\n",
    "        # Scale the image's height and width\n",
    "        size = np.array(image.size) * fact\n",
    "        # The size is now floating-point because it was scaled.\n",
    "        # PIL requires the size to be integers.\n",
    "        size = size.astype(int)\n",
    "        # Resize the image\n",
    "        image = image.resize(size, Image.LANCZOS)\n",
    "    # Convert to numpy floating-point array.\n",
    "    return np.float32(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to save image as a jpeg for image as numpy array, pixel between 0 to 255\n",
    "def save_image(image, filename):\n",
    "    # cliping the the pixel-values to have in 0 and 255\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    # Conversion to bytes\n",
    "    image = image.astype(np.uint8)\n",
    "    # Write the file in jpeg\n",
    "    with open(filename, 'wb') as file:\n",
    "        Image.fromarray(image).save(file, 'jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to plot image as a jpeg for image as numpy array, pixel between 0 to 255\n",
    "def plot_image_big(image):\n",
    "    # cliping the the pixel-values to have in 0 and 255\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    # Conversion to bytes\n",
    "    image = image.astype(np.uint8)\n",
    "    # Converting to a PIL-image and display\n",
    "    display(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to plot the tyle image, input image, and final image\n",
    "def plot_images(input_image, style_image, final_image):\n",
    "    # Create figure with sub-plots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "    # Adjust vertical spacing\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    # Use interpolation to smooth pixels\n",
    "    # Interpolation \n",
    "    interpolation = 'sinc'\n",
    "    # Plot the style-image\n",
    "    ax = axes.flat[0]\n",
    "    ax.imshow(style_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Style Image\")\n",
    "    \n",
    "    # Plot the Input-image.\n",
    "    # normalizing in [0.0, 1.0] range by dividing with 255\n",
    "    ax = axes.flat[1]\n",
    "    ax.imshow(input_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Input Image\")\n",
    "\n",
    "    # Plot the final image.\n",
    "    ax = axes.flat[2]\n",
    "    ax.imshow(final_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Final Image\")\n",
    "\n",
    "    # Remove ticks from all the plots.\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss function for optimization \n",
    "#calculating the mean square error for a and b\n",
    "def mean_squared_error(a, b):\n",
    "    return tf.reduce_mean(tf.square(a - b))\n",
    "#the loss-function for the input image is the Mean Squared Error of the feature \n",
    "#activations in given layers in model between the imput image and the \n",
    "#final image. When this content loss is minimized, it therefore means\n",
    "#that the final image has feature activations in the given layers that\n",
    "#are very similar to the activations of the input image. Depending on \n",
    "#which layers, it will transfer the contours from the \n",
    "#input image to the finalimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #loss-function for the input image.\n",
    "def create_content_loss(session, model, content_image, layer_ids):\n",
    "    # feed-dict with the content-image.\n",
    "    feed_dict = model.create_feed_dict(image=content_image)\n",
    "    # references to  tensors for layers\n",
    "    layers = model.get_layer_tensors(layer_ids)\n",
    "    # Calculate the output values of those layers when\n",
    "    # feeding the content-image to the model\n",
    "    values = session.run(layers, feed_dict=feed_dict)\n",
    "    # Set the model's graph as the default so we can add\n",
    "    # computational nodes to it\n",
    "    with model.graph.as_default():\n",
    "        # Initialize layer_loss function\n",
    "        layer_losses = []\n",
    "        # each layer and its  values for the content-image\n",
    "        for value, layer in zip(values, layers):\n",
    "            # These are the values that are calculated\n",
    "            # for this layer in the model when inputting\n",
    "            # the content-image.it is a const\n",
    "            value_const = tf.constant(value)\n",
    "            # The loss function for layer is the\n",
    "            # Mean Squared Error between the layer-values\n",
    "            loss = mean_squared_error(layer, value_const)\n",
    "            # Add the loss-function for this layer to list of loss-functions\n",
    "            layer_losses.append(loss)\n",
    "        # The combined loss for all layers is average\n",
    "        total_loss = tf.reduce_mean(layer_losses)    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we like to measure which features in the style layers activate simultaneously \n",
    "#for the style image, and then copy this activation pattern to the final image\n",
    "#We use Gram matrix for the tensors output by the style layers. \n",
    "#The Gram matrix is a matrix of dot products for the vectors of the feature\n",
    "#activations of a style layer.For if an entry in the Gram matrix has a large value, \n",
    "#then it means the two features do activate simultaneously for the given style-image.\n",
    "def gram_matrix(tensor):\n",
    "    shape = tensor.get_shape()\n",
    "    # feature channels for the input tensor,from a convolutional layer with 4-dim\n",
    "    num_channels = int(shape[3])\n",
    "    # Reshape the tensor so it is a 2-dim matrix, means doing flatening\n",
    "    matrix = tf.reshape(tensor, shape=[-1, num_channels])\n",
    "    # getting dot-products of all combinations of the feature-channels as gram\n",
    "    gram = tf.matmul(tf.transpose(matrix), matrix)\n",
    "    return gram                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the loss-function for the style-image.Here we calculate the Mean Squared Error \n",
    "#for the Gram-matrices \n",
    "def create_style_loss(session, model, style_image, layer_ids):\n",
    "    # Create a feed-dict with the style-image.\n",
    "    feed_dict = model.create_feed_dict(image=style_image)\n",
    "    # references to the tensors for the given layers.\n",
    "    layers = model.get_layer_tensors(layer_ids)\n",
    "    # Set the model's graph as the default so we can add\n",
    "    # computational nodes to it. \n",
    "    with model.graph.as_default():\n",
    "        # calculating the Gram-matrices for each of the layers.\n",
    "        gram_layers = [gram_matrix(layer) for layer in layers]\n",
    "        # Calculate the values of Gram-matrices when feeding the style-image \n",
    "        values = session.run(gram_layers, feed_dict=feed_dict)\n",
    "        # Initialize loss-functions\n",
    "        layer_losses = []\n",
    "        # For each Gram-matrix layer and its corresponding values\n",
    "        for value, gram_layer in zip(values, gram_layers):\n",
    "            # Gram-matrix values that calculated for this layer when inputting style image\n",
    "            value_const = tf.constant(value)\n",
    "\n",
    "            # loss-function for this layer: Mean Squared Error between the Gram-matrix values\n",
    "            # for the content and final image\n",
    "            loss = mean_squared_error(gram_layer, value_const)\n",
    "\n",
    "            # Add the loss-function list of loss-functions\n",
    "            layer_losses.append(loss)\n",
    "        # The combined loss for all layers \n",
    "        total_loss = tf.reduce_mean(layer_losses)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss-function for denoising the final image. The algorithm used\n",
    "#Total Variation Denoising(https://en.wikipedia.org/wiki/Total_variation_denoising) \n",
    "#and  it shifts the image one pixel in the x and y axis, calculates the difference from \n",
    "#the original image, takes the absolute value to the difference is a positive number, \n",
    "#and sums over all the pixels in the image. This loss-function suppress some of the \n",
    "#noise in the image\n",
    "def create_denoise_loss(model):\n",
    "    loss = tf.reduce_sum(tf.abs(model.input[:,1:,:,:] - model.input[:,:-1,:,:])) + \\\n",
    "           tf.reduce_sum(tf.abs(model.input[:,:,1:,:] - model.input[:,:,:-1,:]))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final function for gradient descent to find an image that minimizes the\n",
    "#loss functions of the content layers and style layers. From here, final image\n",
    "#will take contours of the input image, and resembles the colours and textures\n",
    "#of the style-image.\n",
    "def style_transfer(content_image, style_image,\n",
    "                   content_layer_ids, style_layer_ids,\n",
    "                   weight_content=1.5, weight_style=10.0,\n",
    "                   weight_denoise=0.3,\n",
    "                   num_iterations=120, step_size=10.0,preserve_color='no'):\n",
    "\n",
    "    # instance of the VGG16-model\n",
    "    vmodel = vgg16.VGG16()\n",
    "    # getting weights and mean pixel form model\n",
    "    vgg_weights, vgg_mean_pixel = vmodel.load_net(VGG_PATH)\n",
    "    # starting tensorflow session \n",
    "    session = tf.InteractiveSession(graph=vmodel.graph)\n",
    "    # writing graph to tensorboard\n",
    "    writer = tf.summary.FileWriter('./tensorboard')\n",
    "    writer.add_graph(session.graph)\n",
    "    # Print the names of the content-layers.\n",
    "    print(\"Input layers:\")\n",
    "    print(vmodel.get_layer_names(content_layer_ids))\n",
    "    print()\n",
    "    # Print the names of the style-layers.\n",
    "    print(\"Style layers:\")\n",
    "    print(vmodel.get_layer_names(style_layer_ids))\n",
    "    print()\n",
    "    # loss-function for the content-layers and -image\n",
    "    loss_content = create_content_loss(session=session,model=vmodel,content_image=content_image,layer_ids=content_layer_ids)\n",
    "    # loss-function for the style-layers and -image.\n",
    "    loss_style = create_style_loss(session=session,model=vmodel,style_image=style_image,layer_ids=style_layer_ids)    \n",
    "    # Create the loss-function for the denoising of final image.\n",
    "    loss_denoise = create_denoise_loss(vmodel)\n",
    "    # TensorFlow variables for adjusting the values of loss-functions.\n",
    "    adj_content = tf.Variable(1e-10, name='adj_content')\n",
    "    adj_style = tf.Variable(1e-10, name='adj_style')\n",
    "    adj_denoise = tf.Variable(1e-10, name='adj_denoise')\n",
    "    # Initialize the adjustment values for the loss-functions\n",
    "    session.run([adj_content.initializer,adj_style.initializer,adj_denoise.initializer])\n",
    "\n",
    "    # updating the adjustment values. \n",
    "    #small value 1e-10 added to avoid the possibility of division by zero.\n",
    "    update_adj_content = adj_content.assign(1.0 / (loss_content + 1e-10))\n",
    "    update_adj_style = adj_style.assign(1.0 / (loss_style + 1e-10))\n",
    "    update_adj_denoise = adj_denoise.assign(1.0 / (loss_denoise + 1e-10))\n",
    "\n",
    "    # This is the weighted loss-function that we will minimize\n",
    "    # below in order to generate the mixed-image.\n",
    "    loss_combined = weight_content * adj_content * loss_content + weight_style * adj_style * loss_style + weight_denoise * adj_denoise * loss_denoise\n",
    "    # mathematical function for the gradient of the combined loss-function \n",
    "    gradient = tf.gradients(loss_combined, vmodel.input)\n",
    "    # tensors list to run in each optimization iteration\n",
    "    run_list = [gradient, update_adj_content, update_adj_style,update_adj_denoise]\n",
    "    # The mixed/inputimage is initialized with random noise.\n",
    "    mixed_image = np.random.rand(*content_image.shape) + 128\n",
    "    for i in range(num_iterations):\n",
    "        # feed-dict with the mixed/input-image\n",
    "        feed_dict = vmodel.create_feed_dict(image=mixed_image)\n",
    "        # calculate the value of gradient\n",
    "        [grad, adj_content_val, adj_style_val, adj_denoise_val] = session.run(run_list, feed_dict=feed_dict)\n",
    "        # Reduce the dimensionality of  gradient\n",
    "        grad = np.squeeze(grad)\n",
    "        # Scale the step-size to the gradient-values\n",
    "        step_size_scaled = step_size / (np.std(grad) + 1e-8)\n",
    "        # Update the image by following the gradient.\n",
    "        mixed_image -= grad * step_size_scaled\n",
    "        # clip image to have valid pixel-values between 0 and 255\n",
    "        mixed_image = np.clip(mixed_image, 0.0, 255.0)\n",
    "        # Display status every 10 iterations\n",
    "        if (i % 100 == 0) or (i == num_iterations - 1):\n",
    "            print()\n",
    "            print(\"Iteration:\", i)\n",
    "            # Print adjustment weights for loss-functions\n",
    "            msg = \"Weight Adj. for Input: {0:.2e}, Style: {1:.2e}, Denoise: {2:.2e}\"\n",
    "            print(msg.format(adj_content_val, adj_style_val, adj_denoise_val))\n",
    "            # to preserve input image color or not\n",
    "            if preserve_color.upper()==('YES'):\n",
    "                output_img1=color_tranfer(mixed_image,content_filename)\n",
    "                # plotting the final image, input image and style image\n",
    "                plot_images(input_image=content_image,style_image=style_image,final_image=output_img1*255)\n",
    "            else:\n",
    "                output_img1=mixed_image\n",
    "                # plotting the final image, input image and style image\n",
    "                plot_images(input_image=content_image,style_image=style_image,final_image=output_img1)\n",
    "            \n",
    "    print()\n",
    "    print(\"Final image:\")\n",
    "    if preserve_color.upper()==('YES'):\n",
    "        plot_image_big(output_img1*255)\n",
    "    else:\n",
    "        plot_image_big(output_img1)\n",
    "    # Close the TensorFlow session\n",
    "    session.close()\n",
    "    #return final_image\n",
    "    return output_img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#store input image in ./images folder and load input image\n",
    "content_filename = 'images/input_image.jpg'\n",
    "content_image = load_image(content_filename, max_size=None)\n",
    "#store style image in ./images folder and load input image\n",
    "style_filename = 'images/style_image1.jpg'\n",
    "style_image = load_image(style_filename, max_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In VGG model we have different layers, so I am using 5th layer with index 4\n",
    "content_layer_ids = [4]\n",
    "#VGG has 13 convolutional layers so I am selecting all of them\n",
    "style_layer_ids = list(range(13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lets see the results by running functions, we are also givings weights of content,style and denoise.\n",
    "#it will take more time on CPU than GPU depending on iterations\n",
    "# this function let you decide to preserve color or not\n",
    "img = style_transfer(content_image=content_image,style_image=style_image,content_layer_ids=content_layer_ids,\n",
    "                     style_layer_ids=style_layer_ids,weight_content=10.5,weight_style=5.5,weight_denoise=0.3,\n",
    "                     num_iterations=400,step_size=10.0,preserve_color='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store input image in ./images folder and load input image\n",
    "content_filename = 'images/MJ.jpg'\n",
    "content_image = load_image(content_filename, max_size=None)\n",
    "#store style image in ./images folder and load input image\n",
    "style_filename = 'images/style_3.jpg'\n",
    "style_image = load_image(style_filename, max_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets see the results by running functions, we are also givings weights of content,style and denoise.\n",
    "#it will take more time on CPU than GPU depending on iterations\n",
    "# this function let you decide to preserve color or not\n",
    "img = style_transfer(content_image=content_image,style_image=style_image,content_layer_ids=content_layer_ids,\n",
    "                     style_layer_ids=style_layer_ids,weight_content=10.5,weight_style=5.5,weight_denoise=0.3,\n",
    "                     num_iterations=100,step_size=10.0,preserve_color='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets see the results by running functions, we are also givings weights of content,style and denoise.\n",
    "#it will take more time on CPU than GPU depending on iterations\n",
    "# this function let you decide to preserve color or not\n",
    "img = style_transfer(content_image=content_image,style_image=style_image,content_layer_ids=content_layer_ids,\n",
    "                     style_layer_ids=style_layer_ids,weight_content=10.5,weight_style=5.5,weight_denoise=0.3,\n",
    "                     num_iterations=100,step_size=10.0,preserve_color='no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store input image in ./images folder and load input image\n",
    "content_filename = 'images/bright.jpg'\n",
    "content_image = load_image(content_filename, max_size=None)\n",
    "#store style image in ./images folder and load input image\n",
    "style_filename = 'images/style_3.jpg'\n",
    "style_image = load_image(style_filename, max_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets see the results by running functions, we are also givings weights of content,style and denoise.\n",
    "#it will take more time on CPU than GPU depending on iterations\n",
    "# this function let you decide to preserve color or not\n",
    "img = style_transfer(content_image=content_image,style_image=style_image,content_layer_ids=content_layer_ids,\n",
    "                     style_layer_ids=style_layer_ids,weight_content=8.5,weight_style=10.5,weight_denoise=0.3,\n",
    "                     num_iterations=100,step_size=10.0\n",
    "                     ,preserve_color='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets see the results by running functions, we are also givings weights of content,style and denoise.\n",
    "#it will take more time on CPU than GPU depending on iterations\n",
    "# this function let you decide to preserve color or not\n",
    "img = style_transfer(content_image=content_image,style_image=style_image,content_layer_ids=content_layer_ids,\n",
    "                     style_layer_ids=style_layer_ids,weight_content=8.5,weight_style=10.5,weight_denoise=0.3,\n",
    "                     num_iterations=100,step_size=10.0\n",
    "                     ,preserve_color='no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store input image in ./images folder and load input image\n",
    "content_filename = 'images/bright.jpg'\n",
    "content_image = load_image(content_filename, max_size=None)\n",
    "#store style image in ./images folder and load input image\n",
    "style_filename = 'images/dark_style.jpg'\n",
    "style_image = load_image(style_filename, max_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets see the results by running functions, we are also givings weights of content,style and denoise.\n",
    "#it will take more time on CPU than GPU depending on iterations\n",
    "# this function let you decide to preserve color or not\n",
    "img = style_transfer(content_image=content_image,style_image=style_image,content_layer_ids=content_layer_ids,\n",
    "                     style_layer_ids=style_layer_ids,weight_content=10.5,weight_style=2.5,weight_denoise=0.3,\n",
    "                     num_iterations=100,step_size=10.0\n",
    "                     ,preserve_color='no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
